{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz9BYpozL4V1",
        "outputId": "b75b4e42-f3fb-4349-cd01-09885dd32313"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install SpeechRecognition\n",
        "!pip install pipwin\n",
        "!pipwin install PyAudio\n",
        "!pip install pyttsx3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GxweCFpFcpYA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/gabriel/Desktop/Hackathon/HackathonChatGPT/env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from llama_index import SimpleDirectoryReader, GPTListIndex, GPTVectorStoreIndex, LLMPredictor, PromptHelper # GPTSimpleVectorIndex\n",
        "from llama_index import ServiceContext, StorageContext, load_index_from_storage\n",
        "from langchain import OpenAI\n",
        "import openai\n",
        "import sys\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EEKfcprVc5jA"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "# !export OPENAI_API_KEY=\"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\"\n",
        "openai.api_key = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wcrBu-iEhn8D"
      },
      "outputs": [],
      "source": [
        "def createVectorIndex(path):\n",
        "  max_input = 4096\n",
        "  tokens = 256\n",
        "  chunk_size = 600\n",
        "  max_chunk_overlap = 20\n",
        "  chunk_overlap_ratio = 1\n",
        "\n",
        "  prompt_helper = PromptHelper(context_window=max_input, num_output=tokens, chunk_overlap_ratio=chunk_overlap_ratio, chunk_size_limit=chunk_size)\n",
        "\n",
        "  # Definir LLM\n",
        "  llmPredictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\", max_tokens=tokens))\n",
        "\n",
        "  # Cargar data\n",
        "  docs = SimpleDirectoryReader(path).load_data()\n",
        "\n",
        "  # Crear vector index\n",
        "  service_context = ServiceContext.from_defaults(llm_predictor=llmPredictor, prompt_helper=prompt_helper)\n",
        "  vectorindex = GPTVectorStoreIndex.from_documents(docs, service_context=service_context)\n",
        "  vectorindex.storage_context.persist()\n",
        "  # vectorindex = GPTVectorStoreIndex(documents=docs, llm_predictor=llmPredictor, prompt_helper=prompt_helper)\n",
        "\n",
        "  return vectorindex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RgD-Rs0odk9p"
      },
      "outputs": [],
      "source": [
        "vectorIndex = createVectorIndex('dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "def extract_text_from_voice():\n",
        "  speech_recognition = sr.Recognizer()\n",
        "  with sr.Microphone() as source:\n",
        "    audio = speech_recognition.listen(source, phrase_time_limit=2)\n",
        "    audio_text = \"\"\n",
        "\n",
        "    try:\n",
        "      audio_text = speech_recognition.recognize_google(audio)\n",
        "      print(audio_text)\n",
        "    except Exception as e:\n",
        "      print(f\"Exception: {str(e)}\")\n",
        "\n",
        "    return audio_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyttsx3\n",
        "\n",
        "engine = pyttsx3.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ySwfJsmZnj5m"
      },
      "outputs": [],
      "source": [
        "def chatbot():\n",
        "  # rebuild storage context\n",
        "  storage_context = StorageContext.from_defaults(persist_dir='./storage')\n",
        "  # load index\n",
        "  vIndex = load_index_from_storage(storage_context)\n",
        "  query_engine = vIndex.as_query_engine()\n",
        "\n",
        "  while True:\n",
        "    # prompt = extract_text_from_voice()\n",
        "    prompt = input('Me: ')\n",
        "    if prompt == 'exit':\n",
        "      break\n",
        "    # response = vIndex.query(prompt, response_mode=\"compact\")\n",
        "    response = query_engine.query(prompt)\n",
        "    engine.say(response)\n",
        "    engine.runAndWait()\n",
        "    print(f\"MJ: {response}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "BfXtgRMBod8y",
        "outputId": "4b24ca46-46a2-4f92-81ac-7812b88b245d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MJ: \n",
            "I am Michael Jordan.\n",
            "\n",
            "MJ: \n",
            "Lebron James is one of the most talented and successful players in the NBA today. He is a great scorer, passer, and defender, and has been a leader on championship teams. He is an all-around player who can do it all on the court. He is a great role model for young players and is an inspiration to many.\n",
            "\n",
            "MJ: \n",
            "I'm doing real good.\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chatbot()\n",
            "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mchatbot\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mMe: \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39m# response = vIndex.query(prompt, response_mode=\"compact\")\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m response \u001b[39m=\u001b[39m query_engine\u001b[39m.\u001b[39;49mquery(prompt)\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMJ: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Desktop/Hackathon/HackathonChatGPT/env/lib/python3.9/site-packages/llama_index/indices/query/base.py:23\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     22\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 23\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n\u001b[1;32m     24\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m~/Desktop/Hackathon/HackathonChatGPT/env/lib/python3.9/site-packages/llama_index/query_engine/retriever_query_engine.py:142\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    137\u001b[0m query_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_event_start(\n\u001b[1;32m    138\u001b[0m     CBEventType\u001b[39m.\u001b[39mQUERY, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query_bundle\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    141\u001b[0m retrieve_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_event_start(CBEventType\u001b[39m.\u001b[39mRETRIEVE)\n\u001b[0;32m--> 142\u001b[0m nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retriever\u001b[39m.\u001b[39;49mretrieve(query_bundle)\n\u001b[1;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_event_end(\n\u001b[1;32m    144\u001b[0m     CBEventType\u001b[39m.\u001b[39mRETRIEVE,\n\u001b[1;32m    145\u001b[0m     payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mNODES: nodes},\n\u001b[1;32m    146\u001b[0m     event_id\u001b[39m=\u001b[39mretrieve_id,\n\u001b[1;32m    147\u001b[0m )\n\u001b[1;32m    149\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_synthesizer\u001b[39m.\u001b[39msynthesize(\n\u001b[1;32m    150\u001b[0m     query_bundle\u001b[39m=\u001b[39mquery_bundle,\n\u001b[1;32m    151\u001b[0m     nodes\u001b[39m=\u001b[39mnodes,\n\u001b[1;32m    152\u001b[0m )\n",
            "File \u001b[0;32m~/Desktop/Hackathon/HackathonChatGPT/env/lib/python3.9/site-packages/llama_index/indices/base_retriever.py:21\u001b[0m, in \u001b[0;36mBaseRetriever.retrieve\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     20\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 21\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retrieve(str_or_query_bundle)\n",
            "File \u001b[0;32m~/Desktop/Hackathon/HackathonChatGPT/env/lib/python3.9/site-packages/llama_index/token_counter/token_counter.py:78\u001b[0m, in \u001b[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_llm_predict\u001b[39m(_self: Any, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     77\u001b[0m     \u001b[39mwith\u001b[39;00m wrapper_logic(_self):\n\u001b[0;32m---> 78\u001b[0m         f_return_val \u001b[39m=\u001b[39m f(_self, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m f_return_val\n",
            "File \u001b[0;32m~/Desktop/Hackathon/HackathonChatGPT/env/lib/python3.9/site-packages/llama_index/indices/vector_store/retrievers/retriever.py:70\u001b[0m, in \u001b[0;36mVectorIndexRetriever._retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vector_store\u001b[39m.\u001b[39mis_embedding_query:\n\u001b[1;32m     68\u001b[0m     \u001b[39mif\u001b[39;00m query_bundle\u001b[39m.\u001b[39membedding \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m         query_bundle\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m (\n\u001b[0;32m---> 70\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_service_context\u001b[39m.\u001b[39;49membed_model\u001b[39m.\u001b[39;49mget_agg_embedding_from_queries(\n\u001b[1;32m     71\u001b[0m                 query_bundle\u001b[39m.\u001b[39;49membedding_strs\n\u001b[1;32m     72\u001b[0m             )\n\u001b[1;32m     73\u001b[0m         )\n\u001b[1;32m     75\u001b[0m query \u001b[39m=\u001b[39m VectorStoreQuery(\n\u001b[1;32m     76\u001b[0m     query_embedding\u001b[39m=\u001b[39mquery_bundle\u001b[39m.\u001b[39membedding,\n\u001b[1;32m     77\u001b[0m     similarity_top_k\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_similarity_top_k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m     filters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filters,\n\u001b[1;32m     83\u001b[0m )\n\u001b[1;32m     84\u001b[0m query_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vector_store\u001b[39m.\u001b[39mquery(query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kwargs)\n",
            "File \u001b[0;32m~/Desktop/Hackathon/HackathonChatGPT/env/lib/python3.9/site-packages/llama_index/embeddings/base.py:94\u001b[0m, in \u001b[0;36mBaseEmbedding.get_agg_embedding_from_queries\u001b[0;34m(self, queries, agg_fn)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_agg_embedding_from_queries\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     queries: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m     91\u001b[0m     agg_fn: Optional[Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, List[\u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     92\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m     93\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get aggregated embedding from multiple queries.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     query_embeddings \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_query_embedding(query) \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m queries]\n\u001b[1;32m     95\u001b[0m     agg_fn \u001b[39m=\u001b[39m agg_fn \u001b[39mor\u001b[39;00m mean_agg\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m agg_fn(query_embeddings)\n",
            "File \u001b[0;32m~/Desktop/Hackathon/HackathonChatGPT/env/lib/python3.9/site-packages/llama_index/embeddings/base.py:94\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_agg_embedding_from_queries\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     queries: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m     91\u001b[0m     agg_fn: Optional[Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, List[\u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     92\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m     93\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get aggregated embedding from multiple queries.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     query_embeddings \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_query_embedding(query) \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m queries]\n\u001b[1;32m     95\u001b[0m     agg_fn \u001b[39m=\u001b[39m agg_fn \u001b[39mor\u001b[39;00m mean_agg\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m agg_fn(query_embeddings)\n",
            "File \u001b[0;32m~/Desktop/Hackathon/HackathonChatGPT/env/lib/python3.9/site-packages/llama_index/embeddings/base.py:78\u001b[0m, in \u001b[0;36mBaseEmbedding.get_query_embedding\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get query embedding.\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m event_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_event_start(CBEventType\u001b[39m.\u001b[39mEMBEDDING)\n\u001b[0;32m---> 78\u001b[0m query_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_query_embedding(query)\n\u001b[1;32m     79\u001b[0m query_tokens_count \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer(query))\n\u001b[1;32m     80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_total_tokens_used \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m query_tokens_count\n",
            "File \u001b[0;32m~/Desktop/Hackathon/HackathonChatGPT/env/lib/python3.9/site-packages/llama_index/embeddings/openai.py:235\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_query_embedding\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_query_embedding\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m    234\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get query embedding.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m     \u001b[39mreturn\u001b[39;00m get_embedding(\n\u001b[1;32m    236\u001b[0m         query,\n\u001b[1;32m    237\u001b[0m         engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery_engine,\n\u001b[1;32m    238\u001b[0m         deployment_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment_name,\n\u001b[1;32m    239\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopenai_kwargs,\n\u001b[1;32m    240\u001b[0m     )\n",
            "File \u001b[0;32m~/Desktop/Hackathon/HackathonChatGPT/env/lib/python3.9/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
            "File \u001b[0;32m~/Desktop/Hackathon/HackathonChatGPT/env/lib/python3.9/site-packages/tenacity/__init__.py:389\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoSleep):\n\u001b[1;32m    388\u001b[0m     retry_state\u001b[39m.\u001b[39mprepare_for_next_attempt()\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msleep(do)\n\u001b[1;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[39mreturn\u001b[39;00m do\n",
            "File \u001b[0;32m~/Desktop/Hackathon/HackathonChatGPT/env/lib/python3.9/site-packages/tenacity/nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msleep\u001b[39m(seconds: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[39m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     time\u001b[39m.\u001b[39;49msleep(seconds)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "chatbot()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
